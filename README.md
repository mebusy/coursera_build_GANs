# GAN

## Week 1 Intro to GANs

- [Pre-trained Model Exploration](https://colab.research.google.com/github/https-deeplearning-ai/GANs-Public/blob/master/C1W1_(Colab)_Pre_trained_model_exploration.ipynb)

- Generative Adversarial Network
    - Generator
        - learns to make *fakes* that look *real*
        - you can think of the generator as a painting forger
        - the generator actually isn't very sophisticated. It doesn't know how to produce real looking artwork. Additionally, the generator isn't allowed to see the real images.
        - feed in a *noise vector*,  output a image
    - Discriminator
        - learns to distinguish *real* from *fake*
        - you can think of the discriminator as an art inspector
        - doesn't know what's ream and what's fake in the beginning, but is allowed to look at the real artwork, jumbled up with the fakes ones as well, and he doesn't know which one is which. That's for him to figure out and learn how to decide.
        - The discriminator is a **classifier**.

- Summary
    - The generator's goal is to fool the discriminator
    - The discriminator's goal is to distinguish between real and fake
    - They learn from the competition with each other
    - At the end, *fakes* look *real*

---

- Generator
    - Learning
        - noise Î¾ â†’ **Generator** â†’ XÌ‚ (that can compose an image) â†’ **Discriminator** â†’ YÌ‚<sub>d</sub> â†’ Cost<sub>output YÌ‚</sub>
        - use Cost to update the parameters Î¸<sub>g</sub> of the Generator.
    - It learns the probability of features X
        - P(X<sub>features</sub> | Y<sub>class</sub>)
        - if you're generating lots of  different classes, you can pass into a class of generator
    - The generator takes as input *noise* (random features)
- loss function: BCE
    - Binary Cross Entropy function, or BCE for short, is used for training GANs. 
    - It's useful for these models, because it's especially designed for classification tasks, where there are 2 categories like, real and fake. 
    - Logistic Regression
- Training GANs: 
    - Distriminator
        - noise Î¾ â†’ **Generator** â†’ XÌ‚ (that can compose an image) **+ X (both real and fake examples)** â†’ **Discriminator** â†’ YÌ‚<sub>d</sub> â†’ Cost<sub>output YÌ‚</sub>
        - use Cost to update the parameters Î¸<sub>d</sub> of the Discriminator.
    - Generator
        - noise Î¾ â†’ **Generator** â†’ XÌ‚ (that can compose an image) â†’ **Discriminator** â†’ YÌ‚<sub>d</sub> â†’ Cost<sub>output YÌ‚</sub>
        - use Cost to update the parameters Î¸<sub>g</sub> of the Generator.
- GANs train in an alternating fashion, it's important to keep in mind that **both models should improve together** and should be kept at similar skill levels from the beginning of training. 
    - if you had a discriminator that is superior than the generator, like super, super good, you'll get predictions from it telling you that all the fake examples are 100% fake. Well, that's not useful for the generator, the generator doesn't know how to improve. Everything just looks super fake, there isn't anything telling it to know which direction to go in.
    - Meanwhile, if you had a superior generator that completely outskills the discriminator, you'll get predictions telling you that all the generated images are 100% real. 



## Week 2 Wasserstein GANs with Gradient Penalty


[GANs for Video](https://colab.research.google.com/github/https-deeplearning-ai/GANs-Public/blob/master/C1W2_Video_Generation_(Optional).ipynb)

[paper: DCGAN](pdfs/DCGAN.1511.06434.pdf)

- Wasserstein Loss
    - W-Loss approximates the Earth Mover's Distance
    - W-Loss helps with mode collapse and vanishing gradient problems

BCE Loss | W-Loss
--- | --- 
Discriminator outputs between 0 and 1 | Critic outputs any number 
-[ ğ”¼(log(d(x))) + ğ”¼(1- log(d(g(z)))) ]  |  ğ”¼(c(x)) - ğ”¼(c(g(z)))





